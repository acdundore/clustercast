{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview clustercast is a Python library for ML-based time series forecasting that supports both single-series (local) forecasting and multi-series (global) forecasting for grouped/hierarchical time series. The \"cluster\" in the name comes from the technique of modeling grouped (clustered) time series with shared traits concurrently. It provides two main forecasting approaches: Direct Forecasting : Trains separate models for each forecast step, using direct multi-step forecasting Recursive Forecasting : Trains a single model and uses its one-step-ahead predictions recursively for multi-step forecasting Key features that differentiate clustercast from other libraries include: Support for grouped time series that share common categories using a global architecture Ability to perform stationarity tests for all time series simultaneously (ADF and KPSS) Differencing and Box-Cox transformations Automatic calculation of lag features Several types of seasonality features (Fourier, one-hot, and ordinal), which all support multiple seasonalities Sample weighting to emphasize recent observations Prediction intervals via Conformal Quantile Regression (with direct forecaster) or bootstrapped residuals (with recursive forecaster) Model-agnostic architecture supporting any custom ML model, with LightGBM as the default Ability to tune the hyperparameters of the built-in LightGBM model Handling of exogenous variables Support of missing data (for both individual features and entire timesteps) Both forecasters share a similar API and handle data preprocessing automatically, making it easy to experiment with different forecasting approaches/parameters with minimal changes to code. The library is designed for flexibility and ease of use, allowing users to quickly implement sophisticated forecasting solutions while maintaining control over model parameters and preprocessing options. Installation pip install clustercast Usage The forecasting models in clustercast are simple to use, but come with many additional parameters allowing the user to implement sophisticated preprocessing techniques with ease. An example using the recursive forecasting class is shown below. In this example, there are 12 distinct time series representing sales over time for different regions and product categories. # imports from clustercast.datasets import load_store_sales from clustercast import RecursiveForecaster # load store sales data data = load_store_sales() print(data) ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 2 2015-01-01 Central Office Supplies 996.408 2 3 2015-01-01 Central Technology 31.200 3 4 2015-01-01 East Furniture 199.004 4 5 2015-01-01 East Office Supplies 112.970 .. .. ... ... ... ... 424 8 2017-12-01 South Office Supplies 5302.324 425 9 2017-12-01 South Technology 2910.754 426 10 2017-12-01 West Furniture 14391.752 427 11 2017-12-01 West Office Supplies 9166.328 428 12 2017-12-01 West Technology 8545.118 [429 rows x 5 columns] # create the forecasting model model = RecursiveForecaster( data=data, # provide the full dataset endog_var='Sales', # the sales column will be forecasted id_var='ID', # indicates the different time series identifier column group_vars=['Region', 'Category'], # group features that differentiate the time series timestep_var='YM', # indicates the timestep column lags=12, # include lags 1 through 12 seasonality_ordinal=[12], # include an ordinal seasonality feature ) # fit the model model.fit() # make predictions out to 12 steps ahead forecast = model.predict(steps=12) print(forecast) ID YM Region Category Forecast 0 1 2018-01-01 Central Furniture 3249.188111 1 2 2018-01-01 Central Office Supplies 2484.753879 2 3 2018-01-01 Central Technology 3015.802614 3 4 2018-01-01 East Furniture 1845.889868 4 5 2018-01-01 East Office Supplies 3785.740747 .. .. ... ... ... ... 139 8 2018-12-01 South Office Supplies 4050.859234 140 9 2018-12-01 South Technology 3080.471316 141 10 2018-12-01 West Furniture 9342.107224 142 11 2018-12-01 West Office Supplies 7727.692540 143 12 2018-12-01 West Technology 9912.039919 [144 rows x 5 columns] The output contains forecasts out to 12 months ahead for each of the 12 unique time series. This is a fairly basic example implementation without much thought put into preprocessing or model tuning. This example can be easily extended and optimized using the built-in functionality in clustercast ! License clustercast was created by Alex Dundore. It is licensed under the terms of the MIT license. Credits and Dependencies clustercast is powered by numpy , pandas , statsmodels , and LightGBM .","title":"Home"},{"location":"#overview","text":"clustercast is a Python library for ML-based time series forecasting that supports both single-series (local) forecasting and multi-series (global) forecasting for grouped/hierarchical time series. The \"cluster\" in the name comes from the technique of modeling grouped (clustered) time series with shared traits concurrently. It provides two main forecasting approaches: Direct Forecasting : Trains separate models for each forecast step, using direct multi-step forecasting Recursive Forecasting : Trains a single model and uses its one-step-ahead predictions recursively for multi-step forecasting Key features that differentiate clustercast from other libraries include: Support for grouped time series that share common categories using a global architecture Ability to perform stationarity tests for all time series simultaneously (ADF and KPSS) Differencing and Box-Cox transformations Automatic calculation of lag features Several types of seasonality features (Fourier, one-hot, and ordinal), which all support multiple seasonalities Sample weighting to emphasize recent observations Prediction intervals via Conformal Quantile Regression (with direct forecaster) or bootstrapped residuals (with recursive forecaster) Model-agnostic architecture supporting any custom ML model, with LightGBM as the default Ability to tune the hyperparameters of the built-in LightGBM model Handling of exogenous variables Support of missing data (for both individual features and entire timesteps) Both forecasters share a similar API and handle data preprocessing automatically, making it easy to experiment with different forecasting approaches/parameters with minimal changes to code. The library is designed for flexibility and ease of use, allowing users to quickly implement sophisticated forecasting solutions while maintaining control over model parameters and preprocessing options.","title":"Overview"},{"location":"#installation","text":"pip install clustercast","title":"Installation"},{"location":"#usage","text":"The forecasting models in clustercast are simple to use, but come with many additional parameters allowing the user to implement sophisticated preprocessing techniques with ease. An example using the recursive forecasting class is shown below. In this example, there are 12 distinct time series representing sales over time for different regions and product categories. # imports from clustercast.datasets import load_store_sales from clustercast import RecursiveForecaster # load store sales data data = load_store_sales() print(data) ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 2 2015-01-01 Central Office Supplies 996.408 2 3 2015-01-01 Central Technology 31.200 3 4 2015-01-01 East Furniture 199.004 4 5 2015-01-01 East Office Supplies 112.970 .. .. ... ... ... ... 424 8 2017-12-01 South Office Supplies 5302.324 425 9 2017-12-01 South Technology 2910.754 426 10 2017-12-01 West Furniture 14391.752 427 11 2017-12-01 West Office Supplies 9166.328 428 12 2017-12-01 West Technology 8545.118 [429 rows x 5 columns] # create the forecasting model model = RecursiveForecaster( data=data, # provide the full dataset endog_var='Sales', # the sales column will be forecasted id_var='ID', # indicates the different time series identifier column group_vars=['Region', 'Category'], # group features that differentiate the time series timestep_var='YM', # indicates the timestep column lags=12, # include lags 1 through 12 seasonality_ordinal=[12], # include an ordinal seasonality feature ) # fit the model model.fit() # make predictions out to 12 steps ahead forecast = model.predict(steps=12) print(forecast) ID YM Region Category Forecast 0 1 2018-01-01 Central Furniture 3249.188111 1 2 2018-01-01 Central Office Supplies 2484.753879 2 3 2018-01-01 Central Technology 3015.802614 3 4 2018-01-01 East Furniture 1845.889868 4 5 2018-01-01 East Office Supplies 3785.740747 .. .. ... ... ... ... 139 8 2018-12-01 South Office Supplies 4050.859234 140 9 2018-12-01 South Technology 3080.471316 141 10 2018-12-01 West Furniture 9342.107224 142 11 2018-12-01 West Office Supplies 7727.692540 143 12 2018-12-01 West Technology 9912.039919 [144 rows x 5 columns] The output contains forecasts out to 12 months ahead for each of the 12 unique time series. This is a fairly basic example implementation without much thought put into preprocessing or model tuning. This example can be easily extended and optimized using the built-in functionality in clustercast !","title":"Usage"},{"location":"#license","text":"clustercast was created by Alex Dundore. It is licensed under the terms of the MIT license.","title":"License"},{"location":"#credits-and-dependencies","text":"clustercast is powered by numpy , pandas , statsmodels , and LightGBM .","title":"Credits and Dependencies"},{"location":"DirectForecaster/","text":"Direct Forecaster The DirectForecaster class implements direct multi-step forecasting, supporting both single-series (local) forecasting and multi-series (global) forecasting for grouped or hierarchical time series. [1] It trains separate models for each forecast horizon, using LightGBM as the default base regressor but allowing for custom ML models. The class handles a variety of time series preprocessing techniques natively, including differencing, Box-Cox transformations, seasonality features, and lag calculations. The DirectForecaster provides both point forecasts and prediction intervals, with optional Conformal Quantile Regression (CQR) for state-of-the-art interval coverage. [2] This flexible forecaster is well-suited for time series with complex patterns and multiple relevant predictors. class clustercast. DirectForecaster ( data, endog_var, id_var, timestep_var, group_vars=[], exog_vars=[], boxcox=1, differencing=False, include_level=True, include_timestep=False, lags=1, sample_weight_halflife=None, seasonality_fourier={}, seasonality_onehot=[], seasonality_ordinal=[], lgbm_kwargs={'verbose': -1}, base_regressor=None ) Parameters Once the model class has been instantiated, the parameters may not be changed. Parameter Description data : pd.DataFrame The input data containing the time series, IDs, timesteps, and any grouping or exogenous variables. endog_var : str The name of the target variable to forecast. id_var : str The name of the column containing unique identifiers for each time series. timestep_var : str The name of the column containing the time steps. The timestep values may either be datetimes, integers, or floats. group_vars : list List of column names containing categorical variables used to group the time series. exog_vars : list List of column names containing exogenous variables to use as predictors. boxcox : float or int The Box-Cox transformation parameter. Use 1 for no transformation. A value of 0 will perform a log transformation. differencing : bool Whether to apply first-order differencing to the target variable. include_level : bool Whether to include the level of the target variable as a feature. When True , this is only included when differencing is applied. include_timestep : bool Whether to include the time step as a feature. If True , an integer index starting at zero is mapped to each unique timestep chronologically and passed to the regressor. Use with caution, as this may make your model prone to overfitting. lags : int or list The number of lags, or a list of specific lag values, to use as features. sample_weight_halflife : int The halflife, in number of timesteps, used to calculate sample weights during the model fit (more recent timesteps have a heavier weight). If None , all samples are weighted equally. seasonality_fourier : dict Dictionary with periods as the keys and number of Fourier terms as values. seasonality_onehot : list List of periods for one-hot encoded seasonality features. seasonality_ordinal : list List of periods for ordinal encoded seasonality features. lgbm_kwargs : dict Additional keyword arguments to pass to LGBMRegressor, if no custom base regressor is used. base_regressor : class Alternative regressor class to use instead of LGBMRegressor. You can create an custom wrapper for any statistical or machine learning regressor if certain criteria are met. See the examples page for more information. Methods .fit ( max_steps=1, alpha=None, cqr_cal_size='auto' ) Creates and fits the forecasting model up to a defined forecast horizon. Trains separate base regressors for each lookahead timestep up to max_steps ahead. Also supports prediction intervals with Conformalized Quantile Regression. Argument Description max_steps : int Maximum number of timesteps ahead to forecast. alpha : float Miscoverage rate for prediction intervals (e.g., 0.05 for 95% intervals). If None , only point forecasts are produced. cqr_cal_size : str , int , or float Size of the calibration set for CQR: - 'auto' : Automatically determine size based on data. Uses a minimum of a full season's data (using the largest season) or 20% of the data. - int : Number of time steps to use. - float : Fraction of the total time steps to use in (0, 1). - None , no CQR calibration is performed and standard quantile regression is used. .predict ( steps=1 ) Generates forecasts and prediction intervals, if applicable. Makes predictions using the trained models for each lookahead timestep up to the specified number of steps ahead. If prediction intervals were enabled during fitting, also generates prediction intervals. Argument Description steps : int Number of steps ahead to forecast. If this value is greater than the maximum number of timesteps the forecaster was trained on during the fit method, the fit method is called again with the lengthened forecast horizon. Returns Description forecasts : pd.DataFrame DataFrame containing the forecasts with columns: - ID variable - Timestep variable - Group variables (if applicable) - 'Forecast': Point forecasts - 'Forecast_{alpha/2}' and 'Forecast_{1-alpha/2}': Lower and upper prediction interval bounds (if alpha was specified during fitting). .stationarity_test ( test='both' ) Tests for stationarity of each time series before and after the model's transformations. Performs either Augmented Dickey-Fuller (ADF) test, Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, or both. The ADF test has a null hypothesis of non-stationarity, while KPSS has a null hypothesis of stationarity. Argument Description test : str Which stationarity test(s) to perform: - 'adf' : Augmented Dickey-Fuller test only - 'kpss' : KPSS test only - 'both' : Both ADF and KPSS tests (default) Returns Description results : pd.DataFrame DataFrame containing test results with columns: - ID variable - 'Raw ADF p-value': ADF test p-value on raw data (optional) - 'Raw KPSS p-value': KPSS test p-value on raw data (optional) - 'Transformed ADF p-value': ADF test p-value after transform (optional) - 'Transformed KPSS p-value': KPSS test p-value after transform (optional) Example # imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster # load store sales data data = load_store_sales() print(data) ID YM Region Category Sales exog_1 exog_2 0 1 2015-01-01 Central Furniture 506.358 1.764052 3.248691 1 2 2015-01-01 Central Office Supplies 996.408 0.400157 -1.223513 2 3 2015-01-01 Central Technology 31.200 0.978738 -1.056344 3 4 2015-01-01 East Furniture 199.004 2.240893 -2.145937 4 5 2015-01-01 East Office Supplies 112.970 1.867558 1.730815 .. .. ... ... ... ... ... ... 424 8 2017-12-01 South Office Supplies 5302.324 -0.130107 4.109248 425 9 2017-12-01 South Technology 2910.754 0.093953 0.106819 426 10 2017-12-01 West Furniture 14391.752 0.943046 -0.958314 427 11 2017-12-01 West Office Supplies 9166.328 -2.739677 0.700334 428 12 2017-12-01 West Technology 8545.118 -0.569312 0.034329 [429 rows x 7 columns] # create the forecasting model model = DirectForecaster( data=data, # provide the full dataset endog_var='Sales', # the sales column will be forecasted id_var='ID', # indicates the different time series identifier column group_vars=['Region', 'Category'], # group features that differentiate the time series timestep_var='YM', # indicates the timestep column exog_vars=['exog_1', 'exog_2'], # indicates the exogenous features to use boxcox=0.5, # boxcox transformation with lambda = 0.5 differencing=False, # do not difference the data include_level=False, # do not include a level feature include_timestep=False, # do not include timestep as a feature lags=12, # include lags 1 through 12 sample_weight_halflife=12, # decay the sample weights seasonality_ordinal=[12], # include an ordinal seasonality feature lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # fit the model with a maximum forecast horizon of 12 steps and a 90% prediction interval model.fit(max_steps=12, alpha=0.10, cqr_cal_size='auto') # make predictions out to 12 steps ahead and show the results forecast = model.predict(steps=12) print(forecast) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3249.188111 1066.250237 13043.484485 1 2 2018-01-01 Central Office Supplies 2484.753879 547.091754 7594.467308 2 3 2018-01-01 Central Technology 3015.802614 215.583127 26040.987499 3 4 2018-01-01 East Furniture 1845.889868 1236.899833 7945.113132 4 5 2018-01-01 East Office Supplies 3785.740747 2163.662184 12222.113959 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5828.376613 1727.170471 6891.555979 140 9 2018-12-01 South Technology 3745.596868 1424.952497 14552.931506 141 10 2018-12-01 West Furniture 8873.877963 1040.328678 20655.901796 142 11 2018-12-01 West Office Supplies 7705.059564 1176.046296 15412.515219 143 12 2018-12-01 West Technology 6902.883468 1191.380446 19659.924231 [144 rows x 7 columns] References [1] Hyndman, Rob J., and George Athanasopoulos. \u201cHierarchical and Grouped Time Series.\u201d Forecasting: Principles and Practice, Otexts, 2021. [2] Romano, Yaniv, et al. Conformalized Quantile Regression. May 2019, doi:10.48550/arXiv.1905.03222.","title":"DirectForecaster"},{"location":"DirectForecaster/#direct-forecaster","text":"The DirectForecaster class implements direct multi-step forecasting, supporting both single-series (local) forecasting and multi-series (global) forecasting for grouped or hierarchical time series. [1] It trains separate models for each forecast horizon, using LightGBM as the default base regressor but allowing for custom ML models. The class handles a variety of time series preprocessing techniques natively, including differencing, Box-Cox transformations, seasonality features, and lag calculations. The DirectForecaster provides both point forecasts and prediction intervals, with optional Conformal Quantile Regression (CQR) for state-of-the-art interval coverage. [2] This flexible forecaster is well-suited for time series with complex patterns and multiple relevant predictors. class clustercast. DirectForecaster ( data, endog_var, id_var, timestep_var, group_vars=[], exog_vars=[], boxcox=1, differencing=False, include_level=True, include_timestep=False, lags=1, sample_weight_halflife=None, seasonality_fourier={}, seasonality_onehot=[], seasonality_ordinal=[], lgbm_kwargs={'verbose': -1}, base_regressor=None )","title":"Direct Forecaster"},{"location":"DirectForecaster/#parameters","text":"Once the model class has been instantiated, the parameters may not be changed. Parameter Description data : pd.DataFrame The input data containing the time series, IDs, timesteps, and any grouping or exogenous variables. endog_var : str The name of the target variable to forecast. id_var : str The name of the column containing unique identifiers for each time series. timestep_var : str The name of the column containing the time steps. The timestep values may either be datetimes, integers, or floats. group_vars : list List of column names containing categorical variables used to group the time series. exog_vars : list List of column names containing exogenous variables to use as predictors. boxcox : float or int The Box-Cox transformation parameter. Use 1 for no transformation. A value of 0 will perform a log transformation. differencing : bool Whether to apply first-order differencing to the target variable. include_level : bool Whether to include the level of the target variable as a feature. When True , this is only included when differencing is applied. include_timestep : bool Whether to include the time step as a feature. If True , an integer index starting at zero is mapped to each unique timestep chronologically and passed to the regressor. Use with caution, as this may make your model prone to overfitting. lags : int or list The number of lags, or a list of specific lag values, to use as features. sample_weight_halflife : int The halflife, in number of timesteps, used to calculate sample weights during the model fit (more recent timesteps have a heavier weight). If None , all samples are weighted equally. seasonality_fourier : dict Dictionary with periods as the keys and number of Fourier terms as values. seasonality_onehot : list List of periods for one-hot encoded seasonality features. seasonality_ordinal : list List of periods for ordinal encoded seasonality features. lgbm_kwargs : dict Additional keyword arguments to pass to LGBMRegressor, if no custom base regressor is used. base_regressor : class Alternative regressor class to use instead of LGBMRegressor. You can create an custom wrapper for any statistical or machine learning regressor if certain criteria are met. See the examples page for more information.","title":"Parameters"},{"location":"DirectForecaster/#methods","text":".fit ( max_steps=1, alpha=None, cqr_cal_size='auto' ) Creates and fits the forecasting model up to a defined forecast horizon. Trains separate base regressors for each lookahead timestep up to max_steps ahead. Also supports prediction intervals with Conformalized Quantile Regression. Argument Description max_steps : int Maximum number of timesteps ahead to forecast. alpha : float Miscoverage rate for prediction intervals (e.g., 0.05 for 95% intervals). If None , only point forecasts are produced. cqr_cal_size : str , int , or float Size of the calibration set for CQR: - 'auto' : Automatically determine size based on data. Uses a minimum of a full season's data (using the largest season) or 20% of the data. - int : Number of time steps to use. - float : Fraction of the total time steps to use in (0, 1). - None , no CQR calibration is performed and standard quantile regression is used. .predict ( steps=1 ) Generates forecasts and prediction intervals, if applicable. Makes predictions using the trained models for each lookahead timestep up to the specified number of steps ahead. If prediction intervals were enabled during fitting, also generates prediction intervals. Argument Description steps : int Number of steps ahead to forecast. If this value is greater than the maximum number of timesteps the forecaster was trained on during the fit method, the fit method is called again with the lengthened forecast horizon. Returns Description forecasts : pd.DataFrame DataFrame containing the forecasts with columns: - ID variable - Timestep variable - Group variables (if applicable) - 'Forecast': Point forecasts - 'Forecast_{alpha/2}' and 'Forecast_{1-alpha/2}': Lower and upper prediction interval bounds (if alpha was specified during fitting). .stationarity_test ( test='both' ) Tests for stationarity of each time series before and after the model's transformations. Performs either Augmented Dickey-Fuller (ADF) test, Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, or both. The ADF test has a null hypothesis of non-stationarity, while KPSS has a null hypothesis of stationarity. Argument Description test : str Which stationarity test(s) to perform: - 'adf' : Augmented Dickey-Fuller test only - 'kpss' : KPSS test only - 'both' : Both ADF and KPSS tests (default) Returns Description results : pd.DataFrame DataFrame containing test results with columns: - ID variable - 'Raw ADF p-value': ADF test p-value on raw data (optional) - 'Raw KPSS p-value': KPSS test p-value on raw data (optional) - 'Transformed ADF p-value': ADF test p-value after transform (optional) - 'Transformed KPSS p-value': KPSS test p-value after transform (optional)","title":"Methods"},{"location":"DirectForecaster/#example","text":"# imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster # load store sales data data = load_store_sales() print(data) ID YM Region Category Sales exog_1 exog_2 0 1 2015-01-01 Central Furniture 506.358 1.764052 3.248691 1 2 2015-01-01 Central Office Supplies 996.408 0.400157 -1.223513 2 3 2015-01-01 Central Technology 31.200 0.978738 -1.056344 3 4 2015-01-01 East Furniture 199.004 2.240893 -2.145937 4 5 2015-01-01 East Office Supplies 112.970 1.867558 1.730815 .. .. ... ... ... ... ... ... 424 8 2017-12-01 South Office Supplies 5302.324 -0.130107 4.109248 425 9 2017-12-01 South Technology 2910.754 0.093953 0.106819 426 10 2017-12-01 West Furniture 14391.752 0.943046 -0.958314 427 11 2017-12-01 West Office Supplies 9166.328 -2.739677 0.700334 428 12 2017-12-01 West Technology 8545.118 -0.569312 0.034329 [429 rows x 7 columns] # create the forecasting model model = DirectForecaster( data=data, # provide the full dataset endog_var='Sales', # the sales column will be forecasted id_var='ID', # indicates the different time series identifier column group_vars=['Region', 'Category'], # group features that differentiate the time series timestep_var='YM', # indicates the timestep column exog_vars=['exog_1', 'exog_2'], # indicates the exogenous features to use boxcox=0.5, # boxcox transformation with lambda = 0.5 differencing=False, # do not difference the data include_level=False, # do not include a level feature include_timestep=False, # do not include timestep as a feature lags=12, # include lags 1 through 12 sample_weight_halflife=12, # decay the sample weights seasonality_ordinal=[12], # include an ordinal seasonality feature lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # fit the model with a maximum forecast horizon of 12 steps and a 90% prediction interval model.fit(max_steps=12, alpha=0.10, cqr_cal_size='auto') # make predictions out to 12 steps ahead and show the results forecast = model.predict(steps=12) print(forecast) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3249.188111 1066.250237 13043.484485 1 2 2018-01-01 Central Office Supplies 2484.753879 547.091754 7594.467308 2 3 2018-01-01 Central Technology 3015.802614 215.583127 26040.987499 3 4 2018-01-01 East Furniture 1845.889868 1236.899833 7945.113132 4 5 2018-01-01 East Office Supplies 3785.740747 2163.662184 12222.113959 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5828.376613 1727.170471 6891.555979 140 9 2018-12-01 South Technology 3745.596868 1424.952497 14552.931506 141 10 2018-12-01 West Furniture 8873.877963 1040.328678 20655.901796 142 11 2018-12-01 West Office Supplies 7705.059564 1176.046296 15412.515219 143 12 2018-12-01 West Technology 6902.883468 1191.380446 19659.924231 [144 rows x 7 columns]","title":"Example"},{"location":"DirectForecaster/#references","text":"[1] Hyndman, Rob J., and George Athanasopoulos. \u201cHierarchical and Grouped Time Series.\u201d Forecasting: Principles and Practice, Otexts, 2021. [2] Romano, Yaniv, et al. Conformalized Quantile Regression. May 2019, doi:10.48550/arXiv.1905.03222.","title":"References"},{"location":"RecursiveForecaster/","text":"Recursive Forecaster The RecursiveForecaster class implements recursive multi-step forecasting, supporting both single-series (local) forecasting and multi-series (global) forecasting for grouped or hierarchical time series. [1] It trains a single model and uses the model's predictions as inputs for subsequent predictions, iterating through the forecast horizon. The class handles a variety of time series preprocessing techniques natively, including differencing, Box-Cox transformations, seasonality features, and lag calculations. The RecursiveForecaster provides both point forecasts and prediction intervals by performing a number of prediction runs with bootstrapped residuals for each series. This flexible forecaster is well-suited for time series with complex patterns and multiple relevant predictors. class clustercast. RecursiveForecaster ( data, endog_var, id_var, timestep_var, group_vars=[], exog_vars=[], boxcox=1, differencing=False, include_level=True, include_timestep=False, lags=1, sample_weight_halflife=None, seasonality_fourier={}, seasonality_onehot=[], seasonality_ordinal=[], lgbm_kwargs={'verbose': -1}, base_regressor=None ) Parameters Once the model class has been instantiated, the parameters may not be changed. Parameter Description data : pd.DataFrame The input data containing the time series, IDs, timesteps, and any grouping or exogenous variables. endog_var : str The name of the target variable to forecast. id_var : str The name of the column containing unique identifiers for each time series. timestep_var : str The name of the column containing the time steps. The timestep values may either be datetimes, integers, or floats. group_vars : list List of column names containing categorical variables used to group the time series. exog_vars : list List of column names containing exogenous variables to use as predictors. boxcox : float or int The Box-Cox transformation parameter. Use 1 for no transformation. A value of 0 will perform a log transformation. differencing : bool Whether to apply first-order differencing to the target variable. include_level : bool Whether to include the level of the target variable as a feature. When True , this is only included when differencing is applied. include_timestep : bool Whether to include the time step as a feature. If True , an integer index starting at zero is mapped to each unique timestep chronologically and passed to the regressor. Use with caution, as this may make your model prone to overfitting. lags : int or list The number of lags, or a list of specific lag values, to use as features. sample_weight_halflife : int The halflife, in number of timesteps, used to calculate sample weights during the model fit (more recent timesteps have a heavier weight). If None , all samples are weighted equally. seasonality_fourier : dict Dictionary with periods as the keys and number of Fourier terms as values. seasonality_onehot : list List of periods for one-hot encoded seasonality features. seasonality_ordinal : list List of periods for ordinal encoded seasonality features. lgbm_kwargs : dict Additional keyword arguments to pass to LGBMRegressor, if no custom base regressor is used. base_regressor : class Alternative regressor class to use instead of LGBMRegressor. You can create an custom wrapper for any statistical or machine learning regressor if certain criteria are met. See the examples page for more information. Methods .fit ( alpha=None ) Creates and fits the forecasting model. Trains a single model that will be used recursively for multi-step forecasting. The model can optionally be used to generate prediction intervals via bootstrapped residuals. Argument Description alpha : float Miscoverage rate for prediction intervals (e.g., 0.05 for 95% intervals). If None , only point forecasts are produced. .predict ( steps=1, exog_data=None, bootstrap_iter=500 ) Generates forecasts for multiple steps ahead. Makes predictions up to the specified number of steps ahead by using one-step-ahead forecasts as inputs for subsequent timesteps. Optionally generates prediction intervals via bootstrapped residuals. Argument Description steps : int Number of steps ahead to forecast. Default is 1 . exog_data : pd.DataFrame Future values of exogenous variables. Must contain the same columns as the exogenous variables used during fitting, along with the ID and timestep variables. Although this is an optional argument, performance may be degraded if exogenous variables were used for training and are not provided for prediction. bootstrap_iter : int Number of bootstrap iterations to use when generating prediction intervals. It is recommended to use a bare minimum of 100 bootstrap iterations. An excessive number of iterations will be computationally intensive. Only used when alpha was specified during fitting. Default is 500 . Returns Description forecasts : pd.DataFrame DataFrame containing the forecasts and optionally prediction intervals for each time series at each forecast horizon. .stationarity_test ( test='both' ) Tests for stationarity of each time series before and after the model's transformations. Performs either Augmented Dickey-Fuller (ADF) test, Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, or both. The ADF test has a null hypothesis of non-stationarity, while KPSS has a null hypothesis of stationarity. Argument Description test : str Which stationarity test(s) to perform: - 'adf' : Augmented Dickey-Fuller test only - 'kpss' : KPSS test only - 'both' : Both ADF and KPSS tests (default) Returns Description results : pd.DataFrame DataFrame containing test results with columns: - ID variable - 'Raw ADF p-value': ADF test p-value on raw data (optional) - 'Raw KPSS p-value': KPSS test p-value on raw data (optional) - 'Transformed ADF p-value': ADF test p-value after transform (optional) - 'Transformed KPSS p-value': KPSS test p-value after transform (optional) Example # imports from clustercast.datasets import load_store_sales from clustercast import RecursiveForecaster # load store sales data data = load_store_sales() print(data) ID YM Region Category Sales exog_1 exog_2 0 1 2015-01-01 Central Furniture 506.358 1.764052 3.248691 1 2 2015-01-01 Central Office Supplies 996.408 0.400157 -1.223513 2 3 2015-01-01 Central Technology 31.200 0.978738 -1.056344 3 4 2015-01-01 East Furniture 199.004 2.240893 -2.145937 4 5 2015-01-01 East Office Supplies 112.970 1.867558 1.730815 .. .. ... ... ... ... ... ... 424 8 2017-12-01 South Office Supplies 5302.324 -0.130107 4.109248 425 9 2017-12-01 South Technology 2910.754 0.093953 0.106819 426 10 2017-12-01 West Furniture 14391.752 0.943046 -0.958314 427 11 2017-12-01 West Office Supplies 9166.328 -2.739677 0.700334 428 12 2017-12-01 West Technology 8545.118 -0.569312 0.034329 [429 rows x 7 columns] # show the future data for the exogenous variables (either known or forecasted) print(future_exog) ID exog_1 exog_2 YM 0 1 1.331587 -1.930131 2018-01-01 1 2 0.715279 2.056548 2018-01-01 2 3 -1.545400 0.457260 2018-01-01 3 4 -0.008384 0.890275 2018-01-01 4 5 0.621336 -2.273204 2018-01-01 .. .. ... ... ... 283 8 2.010783 0.686925 2019-12-01 284 9 -0.096784 3.092061 2019-12-01 285 10 0.422202 1.380162 2019-12-01 286 11 -0.225462 -4.091707 2019-12-01 287 12 -0.637943 0.668934 2019-12-01 [288 rows x 4 columns] # create the forecasting model model = RecursiveForecaster( data=data, # provide the full dataset endog_var='Sales', # the sales column will be forecasted id_var='ID', # indicates the different time series identifier column group_vars=['Region', 'Category'], # group features that differentiate the time series timestep_var='YM', # indicates the timestep column exog_vars=['exog_1', 'exog_2'], # indicates the exogenous features to use boxcox=0.5, # boxcox transformation with lambda = 0.5 differencing=False, # do not difference the data include_level=False, # do not include a level feature include_timestep=False, # do not include timestep as a feature lags=12, # include lags 1 through 12 sample_weight_halflife=12, # decay the sample weights seasonality_ordinal=[12], # include an ordinal seasonality feature lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # fit the model model.fit(alpha=0.10) # make predictions out to 12 steps ahead, pass the future exog data, # and use 500 bootstrap iterations to produce prediction intervals forecast = model.predict(steps=12, exog_data=future_exog, bootstrap_iter=500) print(forecast) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3249.188111 823.035650 5335.111385 1 2 2018-01-01 Central Office Supplies 2484.753879 731.983805 5578.788326 2 3 2018-01-01 Central Technology 3015.802614 1504.573711 13206.456765 3 4 2018-01-01 East Furniture 1845.889868 471.868242 5599.483712 4 5 2018-01-01 East Office Supplies 3785.740747 1534.934691 6936.820658 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 4050.859234 1729.549101 10691.168582 140 9 2018-12-01 South Technology 3080.471316 1000.518659 12845.554813 141 10 2018-12-01 West Furniture 9342.107224 7607.681676 13520.501073 142 11 2018-12-01 West Office Supplies 7727.692540 3622.107959 12863.080882 143 12 2018-12-01 West Technology 9912.039919 5914.329473 12409.999083 [144 rows x 7 columns] References [1] Hyndman, Rob J., and George Athanasopoulos. \u201cHierarchical and Grouped Time Series.\u201d Forecasting: Principles and Practice, Otexts, 2021.","title":"RecursiveForecaster"},{"location":"RecursiveForecaster/#recursive-forecaster","text":"The RecursiveForecaster class implements recursive multi-step forecasting, supporting both single-series (local) forecasting and multi-series (global) forecasting for grouped or hierarchical time series. [1] It trains a single model and uses the model's predictions as inputs for subsequent predictions, iterating through the forecast horizon. The class handles a variety of time series preprocessing techniques natively, including differencing, Box-Cox transformations, seasonality features, and lag calculations. The RecursiveForecaster provides both point forecasts and prediction intervals by performing a number of prediction runs with bootstrapped residuals for each series. This flexible forecaster is well-suited for time series with complex patterns and multiple relevant predictors. class clustercast. RecursiveForecaster ( data, endog_var, id_var, timestep_var, group_vars=[], exog_vars=[], boxcox=1, differencing=False, include_level=True, include_timestep=False, lags=1, sample_weight_halflife=None, seasonality_fourier={}, seasonality_onehot=[], seasonality_ordinal=[], lgbm_kwargs={'verbose': -1}, base_regressor=None )","title":"Recursive Forecaster"},{"location":"RecursiveForecaster/#parameters","text":"Once the model class has been instantiated, the parameters may not be changed. Parameter Description data : pd.DataFrame The input data containing the time series, IDs, timesteps, and any grouping or exogenous variables. endog_var : str The name of the target variable to forecast. id_var : str The name of the column containing unique identifiers for each time series. timestep_var : str The name of the column containing the time steps. The timestep values may either be datetimes, integers, or floats. group_vars : list List of column names containing categorical variables used to group the time series. exog_vars : list List of column names containing exogenous variables to use as predictors. boxcox : float or int The Box-Cox transformation parameter. Use 1 for no transformation. A value of 0 will perform a log transformation. differencing : bool Whether to apply first-order differencing to the target variable. include_level : bool Whether to include the level of the target variable as a feature. When True , this is only included when differencing is applied. include_timestep : bool Whether to include the time step as a feature. If True , an integer index starting at zero is mapped to each unique timestep chronologically and passed to the regressor. Use with caution, as this may make your model prone to overfitting. lags : int or list The number of lags, or a list of specific lag values, to use as features. sample_weight_halflife : int The halflife, in number of timesteps, used to calculate sample weights during the model fit (more recent timesteps have a heavier weight). If None , all samples are weighted equally. seasonality_fourier : dict Dictionary with periods as the keys and number of Fourier terms as values. seasonality_onehot : list List of periods for one-hot encoded seasonality features. seasonality_ordinal : list List of periods for ordinal encoded seasonality features. lgbm_kwargs : dict Additional keyword arguments to pass to LGBMRegressor, if no custom base regressor is used. base_regressor : class Alternative regressor class to use instead of LGBMRegressor. You can create an custom wrapper for any statistical or machine learning regressor if certain criteria are met. See the examples page for more information.","title":"Parameters"},{"location":"RecursiveForecaster/#methods","text":".fit ( alpha=None ) Creates and fits the forecasting model. Trains a single model that will be used recursively for multi-step forecasting. The model can optionally be used to generate prediction intervals via bootstrapped residuals. Argument Description alpha : float Miscoverage rate for prediction intervals (e.g., 0.05 for 95% intervals). If None , only point forecasts are produced. .predict ( steps=1, exog_data=None, bootstrap_iter=500 ) Generates forecasts for multiple steps ahead. Makes predictions up to the specified number of steps ahead by using one-step-ahead forecasts as inputs for subsequent timesteps. Optionally generates prediction intervals via bootstrapped residuals. Argument Description steps : int Number of steps ahead to forecast. Default is 1 . exog_data : pd.DataFrame Future values of exogenous variables. Must contain the same columns as the exogenous variables used during fitting, along with the ID and timestep variables. Although this is an optional argument, performance may be degraded if exogenous variables were used for training and are not provided for prediction. bootstrap_iter : int Number of bootstrap iterations to use when generating prediction intervals. It is recommended to use a bare minimum of 100 bootstrap iterations. An excessive number of iterations will be computationally intensive. Only used when alpha was specified during fitting. Default is 500 . Returns Description forecasts : pd.DataFrame DataFrame containing the forecasts and optionally prediction intervals for each time series at each forecast horizon. .stationarity_test ( test='both' ) Tests for stationarity of each time series before and after the model's transformations. Performs either Augmented Dickey-Fuller (ADF) test, Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, or both. The ADF test has a null hypothesis of non-stationarity, while KPSS has a null hypothesis of stationarity. Argument Description test : str Which stationarity test(s) to perform: - 'adf' : Augmented Dickey-Fuller test only - 'kpss' : KPSS test only - 'both' : Both ADF and KPSS tests (default) Returns Description results : pd.DataFrame DataFrame containing test results with columns: - ID variable - 'Raw ADF p-value': ADF test p-value on raw data (optional) - 'Raw KPSS p-value': KPSS test p-value on raw data (optional) - 'Transformed ADF p-value': ADF test p-value after transform (optional) - 'Transformed KPSS p-value': KPSS test p-value after transform (optional)","title":"Methods"},{"location":"RecursiveForecaster/#example","text":"# imports from clustercast.datasets import load_store_sales from clustercast import RecursiveForecaster # load store sales data data = load_store_sales() print(data) ID YM Region Category Sales exog_1 exog_2 0 1 2015-01-01 Central Furniture 506.358 1.764052 3.248691 1 2 2015-01-01 Central Office Supplies 996.408 0.400157 -1.223513 2 3 2015-01-01 Central Technology 31.200 0.978738 -1.056344 3 4 2015-01-01 East Furniture 199.004 2.240893 -2.145937 4 5 2015-01-01 East Office Supplies 112.970 1.867558 1.730815 .. .. ... ... ... ... ... ... 424 8 2017-12-01 South Office Supplies 5302.324 -0.130107 4.109248 425 9 2017-12-01 South Technology 2910.754 0.093953 0.106819 426 10 2017-12-01 West Furniture 14391.752 0.943046 -0.958314 427 11 2017-12-01 West Office Supplies 9166.328 -2.739677 0.700334 428 12 2017-12-01 West Technology 8545.118 -0.569312 0.034329 [429 rows x 7 columns] # show the future data for the exogenous variables (either known or forecasted) print(future_exog) ID exog_1 exog_2 YM 0 1 1.331587 -1.930131 2018-01-01 1 2 0.715279 2.056548 2018-01-01 2 3 -1.545400 0.457260 2018-01-01 3 4 -0.008384 0.890275 2018-01-01 4 5 0.621336 -2.273204 2018-01-01 .. .. ... ... ... 283 8 2.010783 0.686925 2019-12-01 284 9 -0.096784 3.092061 2019-12-01 285 10 0.422202 1.380162 2019-12-01 286 11 -0.225462 -4.091707 2019-12-01 287 12 -0.637943 0.668934 2019-12-01 [288 rows x 4 columns] # create the forecasting model model = RecursiveForecaster( data=data, # provide the full dataset endog_var='Sales', # the sales column will be forecasted id_var='ID', # indicates the different time series identifier column group_vars=['Region', 'Category'], # group features that differentiate the time series timestep_var='YM', # indicates the timestep column exog_vars=['exog_1', 'exog_2'], # indicates the exogenous features to use boxcox=0.5, # boxcox transformation with lambda = 0.5 differencing=False, # do not difference the data include_level=False, # do not include a level feature include_timestep=False, # do not include timestep as a feature lags=12, # include lags 1 through 12 sample_weight_halflife=12, # decay the sample weights seasonality_ordinal=[12], # include an ordinal seasonality feature lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # fit the model model.fit(alpha=0.10) # make predictions out to 12 steps ahead, pass the future exog data, # and use 500 bootstrap iterations to produce prediction intervals forecast = model.predict(steps=12, exog_data=future_exog, bootstrap_iter=500) print(forecast) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3249.188111 823.035650 5335.111385 1 2 2018-01-01 Central Office Supplies 2484.753879 731.983805 5578.788326 2 3 2018-01-01 Central Technology 3015.802614 1504.573711 13206.456765 3 4 2018-01-01 East Furniture 1845.889868 471.868242 5599.483712 4 5 2018-01-01 East Office Supplies 3785.740747 1534.934691 6936.820658 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 4050.859234 1729.549101 10691.168582 140 9 2018-12-01 South Technology 3080.471316 1000.518659 12845.554813 141 10 2018-12-01 West Furniture 9342.107224 7607.681676 13520.501073 142 11 2018-12-01 West Office Supplies 7727.692540 3622.107959 12863.080882 143 12 2018-12-01 West Technology 9912.039919 5914.329473 12409.999083 [144 rows x 7 columns]","title":"Example"},{"location":"RecursiveForecaster/#references","text":"[1] Hyndman, Rob J., and George Athanasopoulos. \u201cHierarchical and Grouped Time Series.\u201d Forecasting: Principles and Practice, Otexts, 2021.","title":"References"},{"location":"datasets/","text":"Datasets clustercast includes two example datasets that can be imported and used to experiment with the forecasting classes. Airline Passengers Dataset function clustercast.datasets. load_airline_passengers () This function returns the well-known airline passengers dataset as a pandas dataframe. [1] The airline passengers dataset records monthly airline passengers over a period of many years. This dataset is great for learning about time series forecasting because it is non-stationary and exhibits strong seasonality. Example from clustercast.datasets import load_airline_passengers # load in the dataset data = load_airline_passengers() print(data) YM Passengers 0 1949-01-01 112 1 1949-02-01 118 2 1949-03-01 132 3 1949-04-01 129 4 1949-05-01 121 .. ... ... 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 [144 rows x 2 columns] Store Sales Dataset function clustercast.datasets. load_store_sales () This function returns a superstore sales dataset as a pandas dataframe. [2] This dataset is a modified version of an open dataset found on Kaggle, linked in the references below. The store sales dataset is a good example for global forecasting of grouped time series, as there are 12 closely related time series that have shared and overlapping attributes (store region and product category). [3] Example from clustercast.datasets import load_store_sales # load in the dataset data = load_store_sales() print(data) ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 1 2015-02-01 Central Furniture 439.310 2 1 2015-03-01 Central Furniture 3639.290 3 1 2015-04-01 Central Furniture 1468.218 4 1 2015-05-01 Central Furniture 2304.382 .. .. ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 569 12 2018-09-01 West Technology 5045.440 570 12 2018-10-01 West Technology 4651.807 571 12 2018-11-01 West Technology 7584.580 572 12 2018-12-01 West Technology 8064.524 [573 rows x 5 columns] References [1] \u201cAirline Passengers.\u201d Kaggle. Accessed 2025. [2] \u201cSuperstore Sales Dataset.\u201d Kaggle. Accessed 2025. [3] Hyndman, Rob J., and George Athanasopoulos. \u201cHierarchical and Grouped Time Series.\u201d Forecasting: Principles and Practice, Otexts, 2021.","title":".datasets"},{"location":"datasets/#datasets","text":"clustercast includes two example datasets that can be imported and used to experiment with the forecasting classes.","title":"Datasets"},{"location":"datasets/#airline-passengers-dataset","text":"function clustercast.datasets. load_airline_passengers () This function returns the well-known airline passengers dataset as a pandas dataframe. [1] The airline passengers dataset records monthly airline passengers over a period of many years. This dataset is great for learning about time series forecasting because it is non-stationary and exhibits strong seasonality.","title":"Airline Passengers Dataset"},{"location":"datasets/#example","text":"from clustercast.datasets import load_airline_passengers # load in the dataset data = load_airline_passengers() print(data) YM Passengers 0 1949-01-01 112 1 1949-02-01 118 2 1949-03-01 132 3 1949-04-01 129 4 1949-05-01 121 .. ... ... 139 1960-08-01 606 140 1960-09-01 508 141 1960-10-01 461 142 1960-11-01 390 143 1960-12-01 432 [144 rows x 2 columns]","title":"Example"},{"location":"datasets/#store-sales-dataset","text":"function clustercast.datasets. load_store_sales () This function returns a superstore sales dataset as a pandas dataframe. [2] This dataset is a modified version of an open dataset found on Kaggle, linked in the references below. The store sales dataset is a good example for global forecasting of grouped time series, as there are 12 closely related time series that have shared and overlapping attributes (store region and product category). [3]","title":"Store Sales Dataset"},{"location":"datasets/#example_1","text":"from clustercast.datasets import load_store_sales # load in the dataset data = load_store_sales() print(data) ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 1 2015-02-01 Central Furniture 439.310 2 1 2015-03-01 Central Furniture 3639.290 3 1 2015-04-01 Central Furniture 1468.218 4 1 2015-05-01 Central Furniture 2304.382 .. .. ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 569 12 2018-09-01 West Technology 5045.440 570 12 2018-10-01 West Technology 4651.807 571 12 2018-11-01 West Technology 7584.580 572 12 2018-12-01 West Technology 8064.524 [573 rows x 5 columns]","title":"Example"},{"location":"datasets/#references","text":"[1] \u201cAirline Passengers.\u201d Kaggle. Accessed 2025. [2] \u201cSuperstore Sales Dataset.\u201d Kaggle. Accessed 2025. [3] Hyndman, Rob J., and George Athanasopoulos. \u201cHierarchical and Grouped Time Series.\u201d Forecasting: Principles and Practice, Otexts, 2021.","title":"References"},{"location":"example_custom-regressor/","text":"Using a Custom Base Regressor In this example, we will use a custom base regressor with the direct forecasting class on the airline passengers dataset. An XGBoost regressor will be used as it natively supports quantile regression. However, the custom base regressor class could easily be adapted to work with any algorithm that can support quantile regression (i.e. pinball loss). This could include neural networks (pytorch or tensorflow), linear regression, or a number of other model types. Data Preparation # imports from clustercast.datasets import load_airline_passengers from clustercast import DirectForecaster, RecursiveForecaster # load airline passenger data airline_data = load_airline_passengers() airline_data['ID'] = 1 print(airline_data) # only keep data before 1959 for training airline_data_train = airline_data.loc[ airline_data['YM'] < dt.datetime(year=1959, month=1, day=1) ] YM Passengers ID 0 1949-01-01 112 1 1 1949-02-01 118 1 2 1949-03-01 132 1 3 1949-04-01 129 1 4 1949-05-01 121 1 .. ... ... .. 139 1960-08-01 606 1 140 1960-09-01 508 1 141 1960-10-01 461 1 142 1960-11-01 390 1 143 1960-12-01 432 1 [144 rows x 3 columns] # plot the airline data fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers', fontsize=16); Create a Custom Base Regressor In this step, we create a custom regressor class that has the following properties: An \"alpha\" argument during instantiation that defines the quantile for prediction (defaults at 0.50, equivalent to Mean Absolute Error) A standard Scikit-Learn compatible fit method A standard Scikit-Learn compatible predict method This class is essentially a wrapper around XGBRegressor that allows us to pass the quantile regression parameters through the instantiation arguments. It is apparent that this class could be modified to support essentially any ML algorithm. # creating a wrapper for custom base regressor using XGBoost class CustomRegressor(): # provide the quantile as an argument (\"alpha\") when instantiating def __init__(self, alpha=0.5): # create a custom XGBoost regressor using pinball loss with a specified quantile self.regressor = XGBRegressor( objective='reg:quantileerror', quantile_alpha=alpha, # uses the \"alpha\" from class instantiation n_estimators=300, max_depth=8, learning_rate=0.05, reg_lambda=0.05 ) # standard Scikit-Learn fit method def fit(self, X, y): self.regressor.fit(X, y) return self # standard Scikit-Learn predict method def predict(self, X, y=None): return self.regressor.predict(X) Direct Forecaster Now, let's create a direct forecaster with our custom base regressor class. The parameters of the DirectForecaster are similar to the other examples, with the exception that we pass our CustomRegressor class to the base_regressor argument when we create the model. # define the model model = DirectForecaster( data=airline_data_train, endog_var='Passengers', id_var='ID', timestep_var='YM', group_vars=[], exog_vars=[], boxcox=0, differencing=True, lags=12, seasonality_ordinal=[12], base_regressor=CustomRegressor # pass the CustomRegressor class created earlier ) # fit the model with a 90% prediction interval # 24 lookahead models, and automatically calculates CQR calibration set size model.fit(max_steps=24, alpha=0.10, cqr_cal_size='auto') # make predictions out to 2 years ahead direct_preds = model.predict(steps=24) # display some predictions print(direct_preds.head()) ID YM Forecast Forecast_0.050 Forecast_0.950 0 1 1959-01-01 344.578409 282.426612 355.635304 1 1 1959-02-01 319.566975 251.435909 438.184587 2 1 1959-03-01 378.031777 275.022579 442.401312 3 1 1959-04-01 370.820691 270.167072 448.559809 4 1 1959-05-01 373.634456 289.659967 476.273913 # display the predictions, including the prediction intervals fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); sns.lineplot(data=direct_preds, x='YM', y='Forecast', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers: Direct Forecast', fontsize=16); ax.fill_between(x=direct_preds['YM'], y1=direct_preds.iloc[:, -2], y2=direct_preds.iloc[:, -1], alpha=0.2, color='orange');","title":"Using a Custom Base Regressor"},{"location":"example_custom-regressor/#using-a-custom-base-regressor","text":"In this example, we will use a custom base regressor with the direct forecasting class on the airline passengers dataset. An XGBoost regressor will be used as it natively supports quantile regression. However, the custom base regressor class could easily be adapted to work with any algorithm that can support quantile regression (i.e. pinball loss). This could include neural networks (pytorch or tensorflow), linear regression, or a number of other model types.","title":"Using a Custom Base Regressor"},{"location":"example_custom-regressor/#data-preparation","text":"# imports from clustercast.datasets import load_airline_passengers from clustercast import DirectForecaster, RecursiveForecaster # load airline passenger data airline_data = load_airline_passengers() airline_data['ID'] = 1 print(airline_data) # only keep data before 1959 for training airline_data_train = airline_data.loc[ airline_data['YM'] < dt.datetime(year=1959, month=1, day=1) ] YM Passengers ID 0 1949-01-01 112 1 1 1949-02-01 118 1 2 1949-03-01 132 1 3 1949-04-01 129 1 4 1949-05-01 121 1 .. ... ... .. 139 1960-08-01 606 1 140 1960-09-01 508 1 141 1960-10-01 461 1 142 1960-11-01 390 1 143 1960-12-01 432 1 [144 rows x 3 columns] # plot the airline data fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers', fontsize=16);","title":"Data Preparation"},{"location":"example_custom-regressor/#create-a-custom-base-regressor","text":"In this step, we create a custom regressor class that has the following properties: An \"alpha\" argument during instantiation that defines the quantile for prediction (defaults at 0.50, equivalent to Mean Absolute Error) A standard Scikit-Learn compatible fit method A standard Scikit-Learn compatible predict method This class is essentially a wrapper around XGBRegressor that allows us to pass the quantile regression parameters through the instantiation arguments. It is apparent that this class could be modified to support essentially any ML algorithm. # creating a wrapper for custom base regressor using XGBoost class CustomRegressor(): # provide the quantile as an argument (\"alpha\") when instantiating def __init__(self, alpha=0.5): # create a custom XGBoost regressor using pinball loss with a specified quantile self.regressor = XGBRegressor( objective='reg:quantileerror', quantile_alpha=alpha, # uses the \"alpha\" from class instantiation n_estimators=300, max_depth=8, learning_rate=0.05, reg_lambda=0.05 ) # standard Scikit-Learn fit method def fit(self, X, y): self.regressor.fit(X, y) return self # standard Scikit-Learn predict method def predict(self, X, y=None): return self.regressor.predict(X)","title":"Create a Custom Base Regressor"},{"location":"example_custom-regressor/#direct-forecaster","text":"Now, let's create a direct forecaster with our custom base regressor class. The parameters of the DirectForecaster are similar to the other examples, with the exception that we pass our CustomRegressor class to the base_regressor argument when we create the model. # define the model model = DirectForecaster( data=airline_data_train, endog_var='Passengers', id_var='ID', timestep_var='YM', group_vars=[], exog_vars=[], boxcox=0, differencing=True, lags=12, seasonality_ordinal=[12], base_regressor=CustomRegressor # pass the CustomRegressor class created earlier ) # fit the model with a 90% prediction interval # 24 lookahead models, and automatically calculates CQR calibration set size model.fit(max_steps=24, alpha=0.10, cqr_cal_size='auto') # make predictions out to 2 years ahead direct_preds = model.predict(steps=24) # display some predictions print(direct_preds.head()) ID YM Forecast Forecast_0.050 Forecast_0.950 0 1 1959-01-01 344.578409 282.426612 355.635304 1 1 1959-02-01 319.566975 251.435909 438.184587 2 1 1959-03-01 378.031777 275.022579 442.401312 3 1 1959-04-01 370.820691 270.167072 448.559809 4 1 1959-05-01 373.634456 289.659967 476.273913 # display the predictions, including the prediction intervals fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); sns.lineplot(data=direct_preds, x='YM', y='Forecast', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers: Direct Forecast', fontsize=16); ax.fill_between(x=direct_preds['YM'], y1=direct_preds.iloc[:, -2], y2=direct_preds.iloc[:, -1], alpha=0.2, color='orange');","title":"Direct Forecaster"},{"location":"example_exog-features/","text":"Modeling with Exogenous Features In this example, we will use a stores sales dataset to perform a global (multi-series) forecast. The dataset represents sales for different store region and product category combinations from a single store chain over time. There are 12 different time series, each with a different combo of region and category (e.g. East Region Furniture Sales or West Region Technology Sales). A global forecasting model trains on all time series simultaneously. Global models can draw parallels across all time series, whereas the single-series models are siloed to only one. For example, in this case, a global model will be able to understand shared trends between all different product categories in a single region. Similarly, it will also understand shared trends across all regions for a single product category. We will walk through data preparation, then show creation of both recursive and direct forecast models. Data Preparation We will start by joining some exogenous data onto the same sales dataset that was used in the Global Forecasting example. The exogenous dataset contains two new features, both macroeconomic indicators in the US: Unemployment Rate and Consumer Price Index (CPI). # imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster, RecursiveForecaster # load store sales data sales_data = load_store_sales() # join the exogenous data onto the store sales data data = pd.merge(left=sales_data, right=exog_data, how='left', on='YM') print(data) # keep only certain data for training data_train = data.loc[ data['YM'] < dt.datetime(year=2018, month=1, day=1) ] ID YM Region Category Sales Unemployment Rate CPI 0 1 2015-01-01 Central Furniture 506.358 5.7 1.947530 1 1 2015-02-01 Central Furniture 439.310 5.5 1.954495 2 1 2015-03-01 Central Furniture 3639.290 5.4 2.433336 3 1 2015-04-01 Central Furniture 1468.218 5.4 2.962651 4 1 2015-05-01 Central Furniture 2304.382 5.6 2.501291 .. .. ... ... ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 3.8 2.296711 569 12 2018-09-01 West Technology 5045.440 3.7 2.333723 570 12 2018-10-01 West Technology 4651.807 3.8 2.606263 571 12 2018-11-01 West Technology 7584.580 3.8 3.723764 572 12 2018-12-01 West Technology 8064.524 3.9 2.911777 [573 rows x 7 columns] We will display only the first 3 time series (of the 12 total) for brevity. # display the first 3 time series fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) ax[i].grid(axis='both') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1) Direct Forecaster Now, let's create a direct forecaster. We will use similar model parameters to those in the Global Forecasting example. This time, we will also include Unemployment Rate and CPI as exogenous features when we instantiate the model. For a direct forecaster, that is the only thing you need to do for modeling exogenous features! # create the forecasting model model = DirectForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], exog_vars=['Unemployment Rate', 'CPI'], # include exog features timestep_var='YM', lags=12, seasonality_ordinal=[12], ) # fit the model model.fit(max_steps=12, alpha=0.10, cqr_cal_size='auto') # make predictions direct_preds = model.predict(steps=12) print(direct_preds) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3263.294564 732.286329 11748.444901 1 2 2018-01-01 Central Office Supplies 2794.477492 208.984567 12961.443208 2 3 2018-01-01 Central Technology 4380.098196 797.547269 21857.918680 3 4 2018-01-01 East Furniture 3729.807853 1296.053621 9192.842472 4 5 2018-01-01 East Office Supplies 4218.281820 2291.741176 11076.653250 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5887.928413 1277.627392 5035.407014 140 9 2018-12-01 South Technology 4424.659682 898.589906 10710.782462 141 10 2018-12-01 West Furniture 9069.307202 1818.777556 14607.406334 142 11 2018-12-01 West Office Supplies 8040.104658 1916.935088 15490.695424 143 12 2018-12-01 West Technology 7740.399938 1768.470532 18257.377666 [144 rows x 7 columns] # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = direct_preds.loc[direct_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1) Recursive Forecaster For recursive forecasting models, you need to also pass future values of the exogenous variables to the prediction method. These future values may either be forecasted themselves or they may be known a-priori (e.g. what-if scenario modeling, or factors that are controlled by you). The reason that future values must be passed to the predict method is because recursive models only predict one step ahead, then feed those new predictions as inputs for the prediction for the next step ahead. The recursive model only makes predictions for the endogenous variable (not the exogenous ones) so they must be passed to the predict method. We will start by preparing the exogenous dataframe that will be passed to the predict method. This dataframe must have the following columns: The timestep The series ID The exogenous variable values for the corresponding timestep and series ID Because we joined the Unemployment Rate and CPI data to the original store sales dataset, we can just isolate the relevant columns from that dataframe. # get the exogenous feature values out to the end of the forecast horizon future_exog = data[['ID', 'YM', 'Unemployment Rate', 'CPI']].copy() print(future_exog) ID YM Unemployment Rate CPI 0 1 2015-01-01 5.7 1.947530 1 1 2015-02-01 5.5 1.954495 2 1 2015-03-01 5.4 2.433336 3 1 2015-04-01 5.4 2.962651 4 1 2015-05-01 5.6 2.501291 .. .. ... ... ... 568 12 2018-08-01 3.8 2.296711 569 12 2018-09-01 3.7 2.333723 570 12 2018-10-01 3.8 2.606263 571 12 2018-11-01 3.8 3.723764 572 12 2018-12-01 3.9 2.911777 [573 rows x 4 columns] Now, let's create a recursive forecaster model. We will use the same parameters as we did with the direct forecaster. Notice that the future exogenous data is passed to the predict method, also. # create the forecasting model model = RecursiveForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], exog_vars=['Unemployment Rate', 'CPI'], # include exog features timestep_var='YM', lags=12, seasonality_ordinal=[12], ) # fit the model model.fit(alpha=0.10) # make predictions, and include the exogenous feature data recursive_preds = model.predict(steps=12, exog_data=future_exog) print(recursive_preds) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3263.294564 2003.807769 4761.098893 1 2 2018-01-01 Central Office Supplies 2794.477492 1313.254393 6970.375753 2 3 2018-01-01 Central Technology 4380.098196 3115.703771 9829.714225 3 4 2018-01-01 East Furniture 3729.807853 2191.461063 9860.048448 4 5 2018-01-01 East Office Supplies 4218.281820 2928.411646 7529.262034 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5018.833178 3994.733445 9392.034780 140 9 2018-12-01 South Technology 3888.920668 2769.218325 12022.306172 141 10 2018-12-01 West Furniture 7410.047095 6238.034085 10714.565368 142 11 2018-12-01 West Office Supplies 7315.177064 5588.471151 8328.289858 143 12 2018-12-01 West Technology 7628.612253 6616.717556 10676.775578 [144 rows x 7 columns] # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = recursive_preds.loc[recursive_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Modeling with Exogenous Features"},{"location":"example_exog-features/#modeling-with-exogenous-features","text":"In this example, we will use a stores sales dataset to perform a global (multi-series) forecast. The dataset represents sales for different store region and product category combinations from a single store chain over time. There are 12 different time series, each with a different combo of region and category (e.g. East Region Furniture Sales or West Region Technology Sales). A global forecasting model trains on all time series simultaneously. Global models can draw parallels across all time series, whereas the single-series models are siloed to only one. For example, in this case, a global model will be able to understand shared trends between all different product categories in a single region. Similarly, it will also understand shared trends across all regions for a single product category. We will walk through data preparation, then show creation of both recursive and direct forecast models.","title":"Modeling with Exogenous Features"},{"location":"example_exog-features/#data-preparation","text":"We will start by joining some exogenous data onto the same sales dataset that was used in the Global Forecasting example. The exogenous dataset contains two new features, both macroeconomic indicators in the US: Unemployment Rate and Consumer Price Index (CPI). # imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster, RecursiveForecaster # load store sales data sales_data = load_store_sales() # join the exogenous data onto the store sales data data = pd.merge(left=sales_data, right=exog_data, how='left', on='YM') print(data) # keep only certain data for training data_train = data.loc[ data['YM'] < dt.datetime(year=2018, month=1, day=1) ] ID YM Region Category Sales Unemployment Rate CPI 0 1 2015-01-01 Central Furniture 506.358 5.7 1.947530 1 1 2015-02-01 Central Furniture 439.310 5.5 1.954495 2 1 2015-03-01 Central Furniture 3639.290 5.4 2.433336 3 1 2015-04-01 Central Furniture 1468.218 5.4 2.962651 4 1 2015-05-01 Central Furniture 2304.382 5.6 2.501291 .. .. ... ... ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 3.8 2.296711 569 12 2018-09-01 West Technology 5045.440 3.7 2.333723 570 12 2018-10-01 West Technology 4651.807 3.8 2.606263 571 12 2018-11-01 West Technology 7584.580 3.8 3.723764 572 12 2018-12-01 West Technology 8064.524 3.9 2.911777 [573 rows x 7 columns] We will display only the first 3 time series (of the 12 total) for brevity. # display the first 3 time series fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) ax[i].grid(axis='both') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Data Preparation"},{"location":"example_exog-features/#direct-forecaster","text":"Now, let's create a direct forecaster. We will use similar model parameters to those in the Global Forecasting example. This time, we will also include Unemployment Rate and CPI as exogenous features when we instantiate the model. For a direct forecaster, that is the only thing you need to do for modeling exogenous features! # create the forecasting model model = DirectForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], exog_vars=['Unemployment Rate', 'CPI'], # include exog features timestep_var='YM', lags=12, seasonality_ordinal=[12], ) # fit the model model.fit(max_steps=12, alpha=0.10, cqr_cal_size='auto') # make predictions direct_preds = model.predict(steps=12) print(direct_preds) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3263.294564 732.286329 11748.444901 1 2 2018-01-01 Central Office Supplies 2794.477492 208.984567 12961.443208 2 3 2018-01-01 Central Technology 4380.098196 797.547269 21857.918680 3 4 2018-01-01 East Furniture 3729.807853 1296.053621 9192.842472 4 5 2018-01-01 East Office Supplies 4218.281820 2291.741176 11076.653250 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5887.928413 1277.627392 5035.407014 140 9 2018-12-01 South Technology 4424.659682 898.589906 10710.782462 141 10 2018-12-01 West Furniture 9069.307202 1818.777556 14607.406334 142 11 2018-12-01 West Office Supplies 8040.104658 1916.935088 15490.695424 143 12 2018-12-01 West Technology 7740.399938 1768.470532 18257.377666 [144 rows x 7 columns] # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = direct_preds.loc[direct_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Direct Forecaster"},{"location":"example_exog-features/#recursive-forecaster","text":"For recursive forecasting models, you need to also pass future values of the exogenous variables to the prediction method. These future values may either be forecasted themselves or they may be known a-priori (e.g. what-if scenario modeling, or factors that are controlled by you). The reason that future values must be passed to the predict method is because recursive models only predict one step ahead, then feed those new predictions as inputs for the prediction for the next step ahead. The recursive model only makes predictions for the endogenous variable (not the exogenous ones) so they must be passed to the predict method. We will start by preparing the exogenous dataframe that will be passed to the predict method. This dataframe must have the following columns: The timestep The series ID The exogenous variable values for the corresponding timestep and series ID Because we joined the Unemployment Rate and CPI data to the original store sales dataset, we can just isolate the relevant columns from that dataframe. # get the exogenous feature values out to the end of the forecast horizon future_exog = data[['ID', 'YM', 'Unemployment Rate', 'CPI']].copy() print(future_exog) ID YM Unemployment Rate CPI 0 1 2015-01-01 5.7 1.947530 1 1 2015-02-01 5.5 1.954495 2 1 2015-03-01 5.4 2.433336 3 1 2015-04-01 5.4 2.962651 4 1 2015-05-01 5.6 2.501291 .. .. ... ... ... 568 12 2018-08-01 3.8 2.296711 569 12 2018-09-01 3.7 2.333723 570 12 2018-10-01 3.8 2.606263 571 12 2018-11-01 3.8 3.723764 572 12 2018-12-01 3.9 2.911777 [573 rows x 4 columns] Now, let's create a recursive forecaster model. We will use the same parameters as we did with the direct forecaster. Notice that the future exogenous data is passed to the predict method, also. # create the forecasting model model = RecursiveForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], exog_vars=['Unemployment Rate', 'CPI'], # include exog features timestep_var='YM', lags=12, seasonality_ordinal=[12], ) # fit the model model.fit(alpha=0.10) # make predictions, and include the exogenous feature data recursive_preds = model.predict(steps=12, exog_data=future_exog) print(recursive_preds) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3263.294564 2003.807769 4761.098893 1 2 2018-01-01 Central Office Supplies 2794.477492 1313.254393 6970.375753 2 3 2018-01-01 Central Technology 4380.098196 3115.703771 9829.714225 3 4 2018-01-01 East Furniture 3729.807853 2191.461063 9860.048448 4 5 2018-01-01 East Office Supplies 4218.281820 2928.411646 7529.262034 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5018.833178 3994.733445 9392.034780 140 9 2018-12-01 South Technology 3888.920668 2769.218325 12022.306172 141 10 2018-12-01 West Furniture 7410.047095 6238.034085 10714.565368 142 11 2018-12-01 West Office Supplies 7315.177064 5588.471151 8328.289858 143 12 2018-12-01 West Technology 7628.612253 6616.717556 10676.775578 [144 rows x 7 columns] # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = recursive_preds.loc[recursive_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Recursive Forecaster"},{"location":"example_inspecting-data/","text":"Inspecting Transformed Data In this example, we will use the same store sales dataset from other examples. Instead of making predictions, we will use some internal properties of the forecasting models to inspect the transformed training data. It is good practice to manually inspect the transformed dataset before inferencing to ensure it aligns with user expectations. Although this model uses the DirectForecaster class, the same process can be completed for RecursiveForecaster . Data Preparation # imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster, RecursiveForecaster # load store sales data data = load_store_sales() print(data) # keep only certain data for training data_train = data.loc[ data['YM'] < dt.datetime(year=2018, month=1, day=1) ] ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 1 2015-02-01 Central Furniture 439.310 2 1 2015-03-01 Central Furniture 3639.290 3 1 2015-04-01 Central Furniture 1468.218 4 1 2015-05-01 Central Furniture 2304.382 .. .. ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 569 12 2018-09-01 West Technology 5045.440 570 12 2018-10-01 West Technology 4651.807 571 12 2018-11-01 West Technology 7584.580 572 12 2018-12-01 West Technology 8064.524 [573 rows x 5 columns] Direct Forecaster First, we can create the direct forecaster model. We will use the following parameters. Although these are likely not optimal, they will be useful for demonstrating the feature transformations. A Box-Cox transform parameter of 0.5 Differencing Inclusion of a level feature 3 lag features Two ordinal seasonality features: one for 12 months (yearly), and one for 3 months (quarterly) # create the forecasting model model = DirectForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], boxcox=0.5, differencing=True, include_level=True, timestep_var='YM', lags=3, seasonality_ordinal=[12, 3], ) # fit the model; data transformation is performed within the fit method model.fit(max_steps=3) First, let's take a look at the timestep inferred by the model. The forecasting classes work with both datetime and non-datetime timesteps, but it is good practice to inspect. In this case, we are working with monthly data. It appears that the timestep delta inferred by the model is correct. # show the inferred timestep print(model._inferred_timestep) <DateOffset: months=1> Next, let's look at the transformed data. We will sort it by series ID and timestep in order to make it easier to understand, since there are multiple time series in the dataset. There are several things to note about the transformed data: There are multiple columns that begin with an underscore. These will not be used for training, but are included to make it easier for the user to double-check the transformations. In this case, the Sales column is put through a Box-Cox transformation ( _endog_boxcox column). Then, differencing is applied ( _endog_differenced column). The final transformed endogenous variable is stored in the endog column, which will be used for training. The endogenous variable after being Box-Cox transformed is stored in the endog_level column, as specified by the include_level argument. There are 3 lag variables included in the transformed data: endog_lag_1 , endog_lag_2 , and endog_lag_3 . These are lags of the final endog column. Because we specified for the DirectForecaster to fit 3 lookahead models ( max_steps =3), there were 3 target columns calculated: endog_lookahead_1 , endog_lookahead_2 , and endog_lookahead_3 . There are two seasonality columns: season_ordinal_p12 and season_ordinal_p3 . There are onehot-encoded columns for both Region and Category (the two grouping variables). # display the transformed data print(model._data_trans.sort_values(by=['ID', 'YM'])) YM ID Region Category Sales endog _endog_boxcox endog_level _endog_differenced endog_lookahead_1 endog_lookahead_2 endog_lookahead_3 endog_lag_1 endog_lag_2 endog_lag_3 season_ordinal_p12 season_ordinal_p3 Region_Central Region_East Region_South Region_West Category_Furniture Category_Office Supplies Category_Technology 0 2015-01-01 1 Central Furniture 506.358 NaN 43.049218 43.049218 NaN -3.082088 75.620414 31.611542 NaN NaN NaN 1 1 1 0 0 0 1 0 0 12 2015-02-01 1 Central Furniture 439.310 -3.082088 39.967130 39.967130 -3.082088 78.702502 34.693629 54.061657 NaN NaN NaN 2 2 1 0 0 0 1 0 0 24 2015-03-01 1 Central Furniture 3639.290 78.702502 118.669632 118.669632 78.702502 -44.008872 -24.640844 9.307329 -3.082088 NaN NaN 3 3 1 0 0 0 1 0 0 36 2015-04-01 1 Central Furniture 1468.218 -44.008872 74.660759 74.660759 -44.008872 19.368028 53.316202 16.867761 78.702502 -3.082088 NaN 4 1 1 0 0 0 1 0 0 48 2015-05-01 1 Central Furniture 2304.382 19.368028 94.028787 94.028787 19.368028 33.948174 -2.500268 -33.884375 -44.008872 78.702502 -3.082088 5 2 1 0 0 0 1 0 0 .. ... .. ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 383 2017-08-01 12 West Technology 4075.004 -29.493989 125.687180 125.687180 -29.493989 52.117225 -14.274518 18.847880 -39.848737 60.500935 63.291658 8 2 0 0 0 1 0 0 1 395 2017-09-01 12 West Technology 8081.406 52.117225 177.804405 177.804405 52.117225 -66.391742 -33.269344 5.086028 -29.493989 -39.848737 60.500935 9 3 0 0 0 1 0 0 1 407 2017-10-01 12 West Technology 3214.608 -66.391742 111.412662 111.412662 -66.391742 33.122398 71.477770 NaN 52.117225 -29.493989 -39.848737 10 1 0 0 0 1 0 0 1 419 2017-11-01 12 West Technology 5367.131 33.122398 144.535061 144.535061 33.122398 38.355372 NaN NaN -66.391742 52.117225 -29.493989 11 2 0 0 0 1 0 0 1 431 2017-12-01 12 West Technology 8545.118 38.355372 182.890432 182.890432 38.355372 NaN NaN NaN 33.122398 -66.391742 52.117225 12 3 0 0 0 1 0 0 1 [432 rows x 24 columns] As previously mentioned, not all columns in the transformed data will be used for training. If we want to see the columns that will be used for training in the X data, we can do the following. Note that the timestep variable, series IDs, original group variables, and intermediate endogenous variable transformations are not included. # display the columns that will be used for training for c in model._X_cols: print(c) endog endog_level endog_lag_1 endog_lag_2 endog_lag_3 season_ordinal_p12 season_ordinal_p3 Region_Central Region_East Region_South Region_West Category_Furniture Category_Office Supplies Category_Technology","title":"Inspecting Transformed Data"},{"location":"example_inspecting-data/#inspecting-transformed-data","text":"In this example, we will use the same store sales dataset from other examples. Instead of making predictions, we will use some internal properties of the forecasting models to inspect the transformed training data. It is good practice to manually inspect the transformed dataset before inferencing to ensure it aligns with user expectations. Although this model uses the DirectForecaster class, the same process can be completed for RecursiveForecaster .","title":"Inspecting Transformed Data"},{"location":"example_inspecting-data/#data-preparation","text":"# imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster, RecursiveForecaster # load store sales data data = load_store_sales() print(data) # keep only certain data for training data_train = data.loc[ data['YM'] < dt.datetime(year=2018, month=1, day=1) ] ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 1 2015-02-01 Central Furniture 439.310 2 1 2015-03-01 Central Furniture 3639.290 3 1 2015-04-01 Central Furniture 1468.218 4 1 2015-05-01 Central Furniture 2304.382 .. .. ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 569 12 2018-09-01 West Technology 5045.440 570 12 2018-10-01 West Technology 4651.807 571 12 2018-11-01 West Technology 7584.580 572 12 2018-12-01 West Technology 8064.524 [573 rows x 5 columns]","title":"Data Preparation"},{"location":"example_inspecting-data/#direct-forecaster","text":"First, we can create the direct forecaster model. We will use the following parameters. Although these are likely not optimal, they will be useful for demonstrating the feature transformations. A Box-Cox transform parameter of 0.5 Differencing Inclusion of a level feature 3 lag features Two ordinal seasonality features: one for 12 months (yearly), and one for 3 months (quarterly) # create the forecasting model model = DirectForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], boxcox=0.5, differencing=True, include_level=True, timestep_var='YM', lags=3, seasonality_ordinal=[12, 3], ) # fit the model; data transformation is performed within the fit method model.fit(max_steps=3) First, let's take a look at the timestep inferred by the model. The forecasting classes work with both datetime and non-datetime timesteps, but it is good practice to inspect. In this case, we are working with monthly data. It appears that the timestep delta inferred by the model is correct. # show the inferred timestep print(model._inferred_timestep) <DateOffset: months=1> Next, let's look at the transformed data. We will sort it by series ID and timestep in order to make it easier to understand, since there are multiple time series in the dataset. There are several things to note about the transformed data: There are multiple columns that begin with an underscore. These will not be used for training, but are included to make it easier for the user to double-check the transformations. In this case, the Sales column is put through a Box-Cox transformation ( _endog_boxcox column). Then, differencing is applied ( _endog_differenced column). The final transformed endogenous variable is stored in the endog column, which will be used for training. The endogenous variable after being Box-Cox transformed is stored in the endog_level column, as specified by the include_level argument. There are 3 lag variables included in the transformed data: endog_lag_1 , endog_lag_2 , and endog_lag_3 . These are lags of the final endog column. Because we specified for the DirectForecaster to fit 3 lookahead models ( max_steps =3), there were 3 target columns calculated: endog_lookahead_1 , endog_lookahead_2 , and endog_lookahead_3 . There are two seasonality columns: season_ordinal_p12 and season_ordinal_p3 . There are onehot-encoded columns for both Region and Category (the two grouping variables). # display the transformed data print(model._data_trans.sort_values(by=['ID', 'YM'])) YM ID Region Category Sales endog _endog_boxcox endog_level _endog_differenced endog_lookahead_1 endog_lookahead_2 endog_lookahead_3 endog_lag_1 endog_lag_2 endog_lag_3 season_ordinal_p12 season_ordinal_p3 Region_Central Region_East Region_South Region_West Category_Furniture Category_Office Supplies Category_Technology 0 2015-01-01 1 Central Furniture 506.358 NaN 43.049218 43.049218 NaN -3.082088 75.620414 31.611542 NaN NaN NaN 1 1 1 0 0 0 1 0 0 12 2015-02-01 1 Central Furniture 439.310 -3.082088 39.967130 39.967130 -3.082088 78.702502 34.693629 54.061657 NaN NaN NaN 2 2 1 0 0 0 1 0 0 24 2015-03-01 1 Central Furniture 3639.290 78.702502 118.669632 118.669632 78.702502 -44.008872 -24.640844 9.307329 -3.082088 NaN NaN 3 3 1 0 0 0 1 0 0 36 2015-04-01 1 Central Furniture 1468.218 -44.008872 74.660759 74.660759 -44.008872 19.368028 53.316202 16.867761 78.702502 -3.082088 NaN 4 1 1 0 0 0 1 0 0 48 2015-05-01 1 Central Furniture 2304.382 19.368028 94.028787 94.028787 19.368028 33.948174 -2.500268 -33.884375 -44.008872 78.702502 -3.082088 5 2 1 0 0 0 1 0 0 .. ... .. ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 383 2017-08-01 12 West Technology 4075.004 -29.493989 125.687180 125.687180 -29.493989 52.117225 -14.274518 18.847880 -39.848737 60.500935 63.291658 8 2 0 0 0 1 0 0 1 395 2017-09-01 12 West Technology 8081.406 52.117225 177.804405 177.804405 52.117225 -66.391742 -33.269344 5.086028 -29.493989 -39.848737 60.500935 9 3 0 0 0 1 0 0 1 407 2017-10-01 12 West Technology 3214.608 -66.391742 111.412662 111.412662 -66.391742 33.122398 71.477770 NaN 52.117225 -29.493989 -39.848737 10 1 0 0 0 1 0 0 1 419 2017-11-01 12 West Technology 5367.131 33.122398 144.535061 144.535061 33.122398 38.355372 NaN NaN -66.391742 52.117225 -29.493989 11 2 0 0 0 1 0 0 1 431 2017-12-01 12 West Technology 8545.118 38.355372 182.890432 182.890432 38.355372 NaN NaN NaN 33.122398 -66.391742 52.117225 12 3 0 0 0 1 0 0 1 [432 rows x 24 columns] As previously mentioned, not all columns in the transformed data will be used for training. If we want to see the columns that will be used for training in the X data, we can do the following. Note that the timestep variable, series IDs, original group variables, and intermediate endogenous variable transformations are not included. # display the columns that will be used for training for c in model._X_cols: print(c) endog endog_level endog_lag_1 endog_lag_2 endog_lag_3 season_ordinal_p12 season_ordinal_p3 Region_Central Region_East Region_South Region_West Category_Furniture Category_Office Supplies Category_Technology","title":"Direct Forecaster"},{"location":"example_multi-series/","text":"Global (Multi-Series) Forecasting In this example, we will use a stores sales dataset to perform a global (multi-series) forecast. The dataset represents sales for different store region and product category combinations from a single store chain over time. There are 12 different time series, each with a different combo of region and category (e.g. East Region Furniture Sales or West Region Technology Sales). A global forecasting model trains on all time series simultaneously. Global models can draw parallels across all time series, whereas the single-series models are siloed to only one. For example, in this case, a global model will be able to understand shared trends between all different product categories in a single region. Similarly, it will also understand shared trends across all regions for a single product category. We will walk through data preparation, then show creation of both recursive and direct forecast models. Data Preparation # imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster, RecursiveForecaster # load store sales data data = load_store_sales() print(data) # keep only certain data for training data_train = data.loc[ data['YM'] < dt.datetime(year=2018, month=1, day=1) ] ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 1 2015-02-01 Central Furniture 439.310 2 1 2015-03-01 Central Furniture 3639.290 3 1 2015-04-01 Central Furniture 1468.218 4 1 2015-05-01 Central Furniture 2304.382 .. .. ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 569 12 2018-09-01 West Technology 5045.440 570 12 2018-10-01 West Technology 4651.807 571 12 2018-11-01 West Technology 7584.580 572 12 2018-12-01 West Technology 8064.524 [573 rows x 5 columns] We will display only the first 3 time series (of the 12 total) for brevity. # display the first 3 time series fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) ax[i].grid(axis='both') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1) Direct Forecaster Now, let's create a direct forecaster. Of the 12 time series, all but 2 of them pass the 0.05 significance threshold for the ADF stationarity test. Because of this, we will not perform any Box-Cox or differencing transformations to the dataset. We will use the following modeling parameters: 12 lag features (a full year prior) Ordinal seasonality feature that is 12 timesteps long A sample weight halflife of 12 months Custom LightGBM hyperparameters Automatic CQR calibration size during the model fit # create the forecasting model model = DirectForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], timestep_var='YM', lags=12, sample_weight_halflife=12, seasonality_ordinal=[12], lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # check stationarity print(model.stationarity_test(test='adf')) # fit the model model.fit(max_steps=12, alpha=0.10, cqr_cal_size='auto') # make predictions direct_preds = model.predict(steps=12) print(direct_preds) ID Raw ADF p-value Transformed ADF p-value 0 1 3.024806e-06 3.024806e-06 1 2 9.720171e-06 9.720171e-06 2 3 4.743699e-05 4.743699e-05 3 4 4.770512e-05 4.770512e-05 4 5 3.168263e-01 3.168263e-01 5 6 2.386381e-03 2.386381e-03 6 7 7.037006e-10 7.037006e-10 7 8 5.054659e-08 5.054659e-08 8 9 1.703273e-07 1.703273e-07 9 10 9.025448e-03 9.025448e-03 10 11 6.618927e-02 6.618927e-02 11 12 1.278997e-07 1.278997e-07 ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3600.877987 1304.404081 12546.381077 1 2 2018-01-01 Central Office Supplies 1854.132830 758.713625 7989.223754 2 3 2018-01-01 Central Technology 3186.816992 373.205139 22228.241521 3 4 2018-01-01 East Furniture 2012.114413 1355.272540 7146.558209 4 5 2018-01-01 East Office Supplies 4260.039391 2117.299462 12542.677098 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5695.951009 1421.737462 6650.924945 140 9 2018-12-01 South Technology 4235.936202 1164.827973 14433.810182 141 10 2018-12-01 West Furniture 9147.575517 1095.153727 18736.839496 142 11 2018-12-01 West Office Supplies 7922.622796 1330.612234 15289.648935 143 12 2018-12-01 West Technology 7706.907351 1524.306079 17297.574709 [144 rows x 7 columns] As shown in the stationarity test results, the Augmented Dickey-Fuller test shows that the p-value for all but 2 of the series pass the 0.05 significance threshold. Because the data transformations must apply the same to all series in a global model and a vast majority of the series are stationary, we will not apply a transformation. Note that the \"raw\" and \"transformed\" p-values in the stationarity test match, since there were not transformations. # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = direct_preds.loc[direct_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1) Recursive Forecaster Now, let's create a recursive forecaster model. We will use the same parameters as we did with the direct forecaster. # create the forecasting model model = RecursiveForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], timestep_var='YM', lags=12, sample_weight_halflife=12, seasonality_ordinal=[12], lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # fit the model model.fit(alpha=0.10) # make predictions recursive_preds = model.predict(steps=12) print(recursive_preds) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3600.877987 1862.175399 4532.177408 1 2 2018-01-01 Central Office Supplies 1854.132830 -649.050239 5697.865943 2 3 2018-01-01 Central Technology 3186.816992 1846.423599 14436.654395 3 4 2018-01-01 East Furniture 2012.114413 253.129934 5767.364540 4 5 2018-01-01 East Office Supplies 4260.039391 2817.869584 7339.969946 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5016.456907 3244.679117 6785.770083 140 9 2018-12-01 South Technology 3063.623135 1636.479677 10877.299013 141 10 2018-12-01 West Furniture 10457.080657 8535.693350 13387.175556 142 11 2018-12-01 West Office Supplies 7680.674792 5173.239341 10100.742047 143 12 2018-12-01 West Technology 9337.751909 6797.900776 11783.540552 [144 rows x 7 columns] # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = recursive_preds.loc[recursive_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Global (Multi-Series) Forecasting"},{"location":"example_multi-series/#global-multi-series-forecasting","text":"In this example, we will use a stores sales dataset to perform a global (multi-series) forecast. The dataset represents sales for different store region and product category combinations from a single store chain over time. There are 12 different time series, each with a different combo of region and category (e.g. East Region Furniture Sales or West Region Technology Sales). A global forecasting model trains on all time series simultaneously. Global models can draw parallels across all time series, whereas the single-series models are siloed to only one. For example, in this case, a global model will be able to understand shared trends between all different product categories in a single region. Similarly, it will also understand shared trends across all regions for a single product category. We will walk through data preparation, then show creation of both recursive and direct forecast models.","title":"Global (Multi-Series) Forecasting"},{"location":"example_multi-series/#data-preparation","text":"# imports from clustercast.datasets import load_store_sales from clustercast import DirectForecaster, RecursiveForecaster # load store sales data data = load_store_sales() print(data) # keep only certain data for training data_train = data.loc[ data['YM'] < dt.datetime(year=2018, month=1, day=1) ] ID YM Region Category Sales 0 1 2015-01-01 Central Furniture 506.358 1 1 2015-02-01 Central Furniture 439.310 2 1 2015-03-01 Central Furniture 3639.290 3 1 2015-04-01 Central Furniture 1468.218 4 1 2015-05-01 Central Furniture 2304.382 .. .. ... ... ... ... 568 12 2018-08-01 West Technology 6230.788 569 12 2018-09-01 West Technology 5045.440 570 12 2018-10-01 West Technology 4651.807 571 12 2018-11-01 West Technology 7584.580 572 12 2018-12-01 West Technology 8064.524 [573 rows x 5 columns] We will display only the first 3 time series (of the 12 total) for brevity. # display the first 3 time series fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) ax[i].grid(axis='both') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Data Preparation"},{"location":"example_multi-series/#direct-forecaster","text":"Now, let's create a direct forecaster. Of the 12 time series, all but 2 of them pass the 0.05 significance threshold for the ADF stationarity test. Because of this, we will not perform any Box-Cox or differencing transformations to the dataset. We will use the following modeling parameters: 12 lag features (a full year prior) Ordinal seasonality feature that is 12 timesteps long A sample weight halflife of 12 months Custom LightGBM hyperparameters Automatic CQR calibration size during the model fit # create the forecasting model model = DirectForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], timestep_var='YM', lags=12, sample_weight_halflife=12, seasonality_ordinal=[12], lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # check stationarity print(model.stationarity_test(test='adf')) # fit the model model.fit(max_steps=12, alpha=0.10, cqr_cal_size='auto') # make predictions direct_preds = model.predict(steps=12) print(direct_preds) ID Raw ADF p-value Transformed ADF p-value 0 1 3.024806e-06 3.024806e-06 1 2 9.720171e-06 9.720171e-06 2 3 4.743699e-05 4.743699e-05 3 4 4.770512e-05 4.770512e-05 4 5 3.168263e-01 3.168263e-01 5 6 2.386381e-03 2.386381e-03 6 7 7.037006e-10 7.037006e-10 7 8 5.054659e-08 5.054659e-08 8 9 1.703273e-07 1.703273e-07 9 10 9.025448e-03 9.025448e-03 10 11 6.618927e-02 6.618927e-02 11 12 1.278997e-07 1.278997e-07 ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3600.877987 1304.404081 12546.381077 1 2 2018-01-01 Central Office Supplies 1854.132830 758.713625 7989.223754 2 3 2018-01-01 Central Technology 3186.816992 373.205139 22228.241521 3 4 2018-01-01 East Furniture 2012.114413 1355.272540 7146.558209 4 5 2018-01-01 East Office Supplies 4260.039391 2117.299462 12542.677098 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5695.951009 1421.737462 6650.924945 140 9 2018-12-01 South Technology 4235.936202 1164.827973 14433.810182 141 10 2018-12-01 West Furniture 9147.575517 1095.153727 18736.839496 142 11 2018-12-01 West Office Supplies 7922.622796 1330.612234 15289.648935 143 12 2018-12-01 West Technology 7706.907351 1524.306079 17297.574709 [144 rows x 7 columns] As shown in the stationarity test results, the Augmented Dickey-Fuller test shows that the p-value for all but 2 of the series pass the 0.05 significance threshold. Because the data transformations must apply the same to all series in a global model and a vast majority of the series are stationary, we will not apply a transformation. Note that the \"raw\" and \"transformed\" p-values in the stationarity test match, since there were not transformations. # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = direct_preds.loc[direct_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Direct Forecaster"},{"location":"example_multi-series/#recursive-forecaster","text":"Now, let's create a recursive forecaster model. We will use the same parameters as we did with the direct forecaster. # create the forecasting model model = RecursiveForecaster( data=data_train, endog_var='Sales', id_var='ID', group_vars=['Region', 'Category'], timestep_var='YM', lags=12, sample_weight_halflife=12, seasonality_ordinal=[12], lgbm_kwargs={'n_estimators': 300, 'learning_rate': 0.03, 'max_depth': 30, 'reg_lambda': 0.03, 'verbose':-1}, ) # fit the model model.fit(alpha=0.10) # make predictions recursive_preds = model.predict(steps=12) print(recursive_preds) ID YM Region Category Forecast Forecast_0.050 Forecast_0.950 0 1 2018-01-01 Central Furniture 3600.877987 1862.175399 4532.177408 1 2 2018-01-01 Central Office Supplies 1854.132830 -649.050239 5697.865943 2 3 2018-01-01 Central Technology 3186.816992 1846.423599 14436.654395 3 4 2018-01-01 East Furniture 2012.114413 253.129934 5767.364540 4 5 2018-01-01 East Office Supplies 4260.039391 2817.869584 7339.969946 .. .. ... ... ... ... ... ... 139 8 2018-12-01 South Office Supplies 5016.456907 3244.679117 6785.770083 140 9 2018-12-01 South Technology 3063.623135 1636.479677 10877.299013 141 10 2018-12-01 West Furniture 10457.080657 8535.693350 13387.175556 142 11 2018-12-01 West Office Supplies 7680.674792 5173.239341 10100.742047 143 12 2018-12-01 West Technology 9337.751909 6797.900776 11783.540552 [144 rows x 7 columns] # display the first 3 time series forecasts fig, ax = plt.subplots(3, 1, figsize=(9, 9)) ax = np.ravel(ax) for i in range(3): ts_known = data.loc[data['ID'] == i + 1] ts_pred = recursive_preds.loc[recursive_preds['ID'] == i + 1] sns.lineplot(data=ts_known, x='YM', y='Sales', ax=ax[i]) sns.lineplot(data=ts_pred, x='YM', y='Forecast', ax=ax[i]) ax[i].grid(axis='both') ax[i].fill_between(x=ts_pred['YM'], y1=ts_pred.iloc[:, -2], y2=ts_pred.iloc[:, -1], alpha=0.2, color='orange') ax[i].set_title(f'{ts_pred['Region'].iloc[0]}, {ts_pred['Category'].iloc[0]}') fig.tight_layout(pad=1)","title":"Recursive Forecaster"},{"location":"example_single-series/","text":"Local (Single-Series) Forecasting In this example, we will use the well-known airline passengers dataset to perform a simple single-series forecast. We will walk through data preparation, then show creation of both recursive and direct forecast models. Data Preparation # imports from clustercast.datasets import load_airline_passengers from clustercast import DirectForecaster, RecursiveForecaster # load airline passenger data airline_data = load_airline_passengers() airline_data['ID'] = 1 print(airline_data) # only keep data before 1959 for training airline_data_train = airline_data.loc[ airline_data['YM'] < dt.datetime(year=1959, month=1, day=1) ] YM Passengers ID 0 1949-01-01 112 1 1 1949-02-01 118 1 2 1949-03-01 132 1 3 1949-04-01 129 1 4 1949-05-01 121 1 .. ... ... .. 139 1960-08-01 606 1 140 1960-09-01 508 1 141 1960-10-01 461 1 142 1960-11-01 390 1 143 1960-12-01 432 1 [144 rows x 3 columns] # plot the airline data fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers', fontsize=16); Direct Forecaster Now, let's create a direct forecaster. Because the time series is non-stationary, we will take the log of the series and then include first order differencing. We'll also use 12 lag features (a full year prior) and apply an ordinal seasonality feature that is 12 timesteps long. # define the model model = DirectForecaster( data=airline_data_train, endog_var='Passengers', id_var='ID', timestep_var='YM', group_vars=[], exog_vars=[], boxcox=0, differencing=True, lags=12, seasonality_ordinal=[12], ) # show stationarity test print(model.stationarity_test(test='adf')) # fit the model with a 90% prediction interval # 24 lookahead models, with 4 years of CQR calibration data model.fit(max_steps=24, alpha=0.10, cqr_cal_size=48) # make predictions out to 2 years ahead direct_preds = model.predict(steps=24) # display some predictions print(direct_preds.head()) ID Raw ADF p-value Transformed ADF p-value 0 1 0.826794 0.158228 ID YM Forecast Forecast_0.050 Forecast_0.950 0 1 1959-01-01 343.240852 318.309103 356.097399 1 1 1959-02-01 327.078275 291.315758 381.538205 2 1 1959-03-01 363.945743 296.112474 416.552425 3 1 1959-04-01 356.755783 314.271979 450.546157 4 1 1959-05-01 381.087484 333.984742 431.445450 As shown in the stationarity test results, the Augmented Dickey-Fuller test shows that the p-value after the data transformations is much closer to stationary than before, but it is still not quite passing the significance threshold of 0.05. That is okay for this example. # display the predictions, including the prediction intervals fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); sns.lineplot(data=direct_preds, x='YM', y='Forecast', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers: Direct Forecast', fontsize=16); ax.fill_between(x=direct_preds['YM'], y1=direct_preds.iloc[:, -2], y2=direct_preds.iloc[:, -1], alpha=0.2, color='orange'); Recursive Forecaster Now, let's create a recursive forecaster model. We will use the same parameters as we did with the direct forecaster. # define the model model = RecursiveForecaster( data=airline_data_train, endog_var='Passengers', id_var='ID', timestep_var='YM', group_vars=[], exog_vars=[], boxcox=0, differencing=True, lags=12, seasonality_ordinal=[12], ) # fit the model with a 90% prediction interval model.fit(alpha=0.10) # make predictions out to 2 years ahead recursive_preds = model.predict(steps=24) # display some predictions print(recursive_preds.head()) ID YM Forecast Forecast_0.050 Forecast_0.950 0 1 1959-01-01 343.240852 323.130861 372.400399 1 1 1959-02-01 325.689354 297.579253 356.483323 2 1 1959-03-01 371.489560 333.035457 410.702360 3 1 1959-04-01 356.554110 317.830452 396.871775 4 1 1959-05-01 368.268217 320.869205 411.887196 # display the predictions, including the prediction intervals fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); sns.lineplot(data=recursive_preds, x='YM', y='Forecast', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers: Recursive Forecast', fontsize=16); ax.fill_between(x=recursive_preds['YM'], y1=recursive_preds.iloc[:, -2], y2=recursive_preds.iloc[:, -1], alpha=0.2, color='orange');","title":"Local (Single-Series) Forecasting"},{"location":"example_single-series/#local-single-series-forecasting","text":"In this example, we will use the well-known airline passengers dataset to perform a simple single-series forecast. We will walk through data preparation, then show creation of both recursive and direct forecast models.","title":"Local (Single-Series) Forecasting"},{"location":"example_single-series/#data-preparation","text":"# imports from clustercast.datasets import load_airline_passengers from clustercast import DirectForecaster, RecursiveForecaster # load airline passenger data airline_data = load_airline_passengers() airline_data['ID'] = 1 print(airline_data) # only keep data before 1959 for training airline_data_train = airline_data.loc[ airline_data['YM'] < dt.datetime(year=1959, month=1, day=1) ] YM Passengers ID 0 1949-01-01 112 1 1 1949-02-01 118 1 2 1949-03-01 132 1 3 1949-04-01 129 1 4 1949-05-01 121 1 .. ... ... .. 139 1960-08-01 606 1 140 1960-09-01 508 1 141 1960-10-01 461 1 142 1960-11-01 390 1 143 1960-12-01 432 1 [144 rows x 3 columns] # plot the airline data fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers', fontsize=16);","title":"Data Preparation"},{"location":"example_single-series/#direct-forecaster","text":"Now, let's create a direct forecaster. Because the time series is non-stationary, we will take the log of the series and then include first order differencing. We'll also use 12 lag features (a full year prior) and apply an ordinal seasonality feature that is 12 timesteps long. # define the model model = DirectForecaster( data=airline_data_train, endog_var='Passengers', id_var='ID', timestep_var='YM', group_vars=[], exog_vars=[], boxcox=0, differencing=True, lags=12, seasonality_ordinal=[12], ) # show stationarity test print(model.stationarity_test(test='adf')) # fit the model with a 90% prediction interval # 24 lookahead models, with 4 years of CQR calibration data model.fit(max_steps=24, alpha=0.10, cqr_cal_size=48) # make predictions out to 2 years ahead direct_preds = model.predict(steps=24) # display some predictions print(direct_preds.head()) ID Raw ADF p-value Transformed ADF p-value 0 1 0.826794 0.158228 ID YM Forecast Forecast_0.050 Forecast_0.950 0 1 1959-01-01 343.240852 318.309103 356.097399 1 1 1959-02-01 327.078275 291.315758 381.538205 2 1 1959-03-01 363.945743 296.112474 416.552425 3 1 1959-04-01 356.755783 314.271979 450.546157 4 1 1959-05-01 381.087484 333.984742 431.445450 As shown in the stationarity test results, the Augmented Dickey-Fuller test shows that the p-value after the data transformations is much closer to stationary than before, but it is still not quite passing the significance threshold of 0.05. That is okay for this example. # display the predictions, including the prediction intervals fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); sns.lineplot(data=direct_preds, x='YM', y='Forecast', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers: Direct Forecast', fontsize=16); ax.fill_between(x=direct_preds['YM'], y1=direct_preds.iloc[:, -2], y2=direct_preds.iloc[:, -1], alpha=0.2, color='orange');","title":"Direct Forecaster"},{"location":"example_single-series/#recursive-forecaster","text":"Now, let's create a recursive forecaster model. We will use the same parameters as we did with the direct forecaster. # define the model model = RecursiveForecaster( data=airline_data_train, endog_var='Passengers', id_var='ID', timestep_var='YM', group_vars=[], exog_vars=[], boxcox=0, differencing=True, lags=12, seasonality_ordinal=[12], ) # fit the model with a 90% prediction interval model.fit(alpha=0.10) # make predictions out to 2 years ahead recursive_preds = model.predict(steps=24) # display some predictions print(recursive_preds.head()) ID YM Forecast Forecast_0.050 Forecast_0.950 0 1 1959-01-01 343.240852 323.130861 372.400399 1 1 1959-02-01 325.689354 297.579253 356.483323 2 1 1959-03-01 371.489560 333.035457 410.702360 3 1 1959-04-01 356.554110 317.830452 396.871775 4 1 1959-05-01 368.268217 320.869205 411.887196 # display the predictions, including the prediction intervals fig, ax = plt.subplots(figsize=(10, 4)); sns.lineplot(data=airline_data, x='YM', y='Passengers', ax=ax); sns.lineplot(data=recursive_preds, x='YM', y='Forecast', ax=ax); ax.grid(axis='both'); ax.set_title('Airline Passengers: Recursive Forecast', fontsize=16); ax.fill_between(x=recursive_preds['YM'], y1=recursive_preds.iloc[:, -2], y2=recursive_preds.iloc[:, -1], alpha=0.2, color='orange');","title":"Recursive Forecaster"}]}